"""
è„‘ç½‘ç»œçŠ¶æ€åˆ†ç±»å™¨ä¸»ç¨‹åº

è¯¥æ¨¡å—å®ç°äº†åŸºäºå›¾å·ç§¯ç½‘ç»œ(GCN)çš„è„‘ç½‘ç»œçŠ¶æ€åˆ†ç±»ç³»ç»Ÿï¼Œç”¨äºåˆ†æå’Œåˆ†ç±»ç¥ç»å…ƒæ´»åŠ¨çŠ¶æ€ã€‚
ä¸»è¦åŠŸèƒ½åŒ…æ‹¬æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•è¿‡ç¨‹çš„å®ç°ã€‚

ä½œè€…: Clade 4
æ—¥æœŸ: 2025å¹´5æœˆ23æ—¥
æ”¹è¿›ç‰ˆæœ¬: å¢åŠ Focal Lossã€æ—©åœæœºåˆ¶ã€å­¦ä¹ ç‡è°ƒåº¦ç­‰å…ˆè¿›è®­ç»ƒæŠ€æœ¯
"""

import torch
import torch.nn.functional as F
import random
import numpy as np
from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report
import logging
import os

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from utils import get_dataset
from model import MultiLayerGCN, AdvancedBrainStateClassifier
from config import config

# é…ç½®æ—¥å¿—ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—é…ç½®æ–¹æ³•ï¼‰
logger = config.setup_logging(config.TRAINING_LOG_FILE, __name__)


class FocalLoss(torch.nn.Module):
    """
    Focal Loss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    ä¸“é—¨è®¾è®¡ç”¨æ¥è§£å†³ç±»åˆ«ä¸å¹³è¡¡å¯¼è‡´çš„è®­ç»ƒé—®é¢˜
    """
    
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        """
        åˆå§‹åŒ–Focal Loss
        
        Parameters
        ----------
        alpha : float
            å¹³è¡¡å› å­ï¼Œç”¨äºè°ƒèŠ‚æ­£è´Ÿæ ·æœ¬çš„æƒé‡
        gamma : float
            èšç„¦å‚æ•°ï¼Œç”¨äºé™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡
        reduction : str
            æŸå¤±èšåˆæ–¹å¼ï¼š'mean', 'sum', 'none'
        """
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        """
        å‰å‘ä¼ æ’­è®¡ç®—Focal Loss
        
        Parameters
        ----------
        inputs : torch.Tensor
            æ¨¡å‹é¢„æµ‹ç»“æœ (logits)
        targets : torch.Tensor
            çœŸå®æ ‡ç­¾
            
        Returns
        -------
        torch.Tensor
            Focal Losså€¼
        """
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss


class EarlyStopping:
    """
    æ—©åœæœºåˆ¶ - é˜²æ­¢è¿‡æ‹Ÿåˆå¹¶èŠ‚çœè®­ç»ƒæ—¶é—´
    """
    
    def __init__(self, patience=10, min_delta=0.001, restore_best_weights=True):
        """
        åˆå§‹åŒ–æ—©åœæœºåˆ¶
        
        Parameters
        ----------
        patience : int
            è€å¿ƒå€¼ï¼Œè¿ç»­å¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹è¿›å°±åœæ­¢
        min_delta : float
            æœ€å°æ”¹è¿›é‡ï¼Œå°äºæ­¤å€¼ä¸è®¤ä¸ºæ˜¯æ”¹è¿›
        restore_best_weights : bool
            æ˜¯å¦æ¢å¤æœ€ä½³æƒé‡
        """
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ
        
        Parameters
        ----------
        val_loss : float
            å½“å‰éªŒè¯æŸå¤±
        model : nn.Module
            å½“å‰æ¨¡å‹
            
        Returns
        -------
        bool
            æ˜¯å¦åº”è¯¥æ—©åœ
        """
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False
    
    def save_checkpoint(self, model):
        """ä¿å­˜æœ€ä½³æ¨¡å‹æƒé‡"""
        self.best_weights = model.state_dict().copy()


class ContrastiveLoss(torch.nn.Module):
    """
    å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•° - åŸºäºInfoNCEæŸå¤±
    
    å¸®åŠ©æ¨¡å‹å­¦ä¹ æ›´å¥½çš„ç‰¹å¾è¡¨ç¤ºï¼Œç‰¹åˆ«é€‚åˆç±»åˆ«ä¸å¹³è¡¡çš„åœºæ™¯
    """
    
    def __init__(self, temperature=0.1):
        super(ContrastiveLoss, self).__init__()
        self.temperature = temperature
        
    def forward(self, embeddings, labels):
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
        
        Parameters
        ----------
        embeddings : torch.Tensor
            ç‰¹å¾åµŒå…¥ [batch_size, feature_dim]
        labels : torch.Tensor
            æ ‡ç­¾ [batch_size]
        """
        batch_size = embeddings.shape[0]
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        embeddings_norm = F.normalize(embeddings, dim=1)
        similarity_matrix = torch.matmul(embeddings_norm, embeddings_norm.T) / self.temperature
        
        # åˆ›å»ºæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬æ©ç 
        labels = labels.unsqueeze(1)
        positive_mask = torch.eq(labels, labels.T).float()
        negative_mask = 1 - positive_mask
        
        # æ’é™¤è‡ªèº«
        positive_mask.fill_diagonal_(0)
        
        # è®¡ç®—å¯¹æ¯”æŸå¤±
        exp_sim = torch.exp(similarity_matrix)
        
        # æ­£æ ·æœ¬æŸå¤±
        positive_sum = torch.sum(exp_sim * positive_mask, dim=1)
        
        # è´Ÿæ ·æœ¬æŸå¤±
        negative_sum = torch.sum(exp_sim * negative_mask, dim=1)
        
        # InfoNCEæŸå¤±
        loss = -torch.log(positive_sum / (positive_sum + negative_sum + 1e-8))
        
        return torch.mean(loss)


class AdaptiveTrainer:
    """
    è‡ªé€‚åº”è®­ç»ƒå™¨ - é›†æˆå¤šç§å…ˆè¿›çš„è®­ç»ƒæŠ€æœ¯
    
    åŸºäº2024å¹´æœ€æ–°ç ”ç©¶çš„è®­ç»ƒç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š
    1. æ··åˆæŸå¤±å‡½æ•°ï¼ˆFocal + Contrastiveï¼‰
    2. è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦
    3. è¯¾ç¨‹å­¦ä¹ 
    4. æ¨¡å‹é›†æˆ
    """
    
    def __init__(self, model, device, config):
        self.model = model.to(device)
        self.device = device
        self.config = config
        
        # æŸå¤±å‡½æ•°
        self.focal_loss = FocalLoss(alpha=1.0, gamma=2.0)
        self.contrastive_loss = ContrastiveLoss(temperature=0.1)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=config.LEARNING_RATE,
            weight_decay=config.WEIGHT_DECAY,
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=15,
            min_lr=1e-6,
            verbose=True
        )
        
        # æ—©åœæœºåˆ¶ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ï¼‰
        if config.USE_EARLY_STOPPING:
            self.early_stopping = EarlyStopping(
                patience=config.EARLY_STOPPING_PATIENCE,
                min_delta=config.EARLY_STOPPING_MIN_DELTA,
                restore_best_weights=config.EARLY_STOPPING_RESTORE_BEST
            )
            logger.info(f"âœ“ æ—©åœæœºåˆ¶å·²å¯ç”¨ - è€å¿ƒå€¼: {config.EARLY_STOPPING_PATIENCE}, æœ€å°æ”¹è¿›: {config.EARLY_STOPPING_MIN_DELTA}")
        else:
            self.early_stopping = None
            logger.info("âœ“ æ—©åœæœºåˆ¶å·²ç¦ç”¨")
        
        # è®­ç»ƒå†å²
        self.train_history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'learning_rates': []
        }
    
    def train_epoch(self, train_loader, epoch):
        """
        è®­ç»ƒä¸€ä¸ªepoch
        """
        self.model.train()
        total_loss = 0
        all_preds = []
        all_labels = []
        
        for batch_idx, data in enumerate(train_loader):
            data = data.to(self.device)
            self.optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ï¼ˆè·å–åµŒå…¥ç”¨äºå¯¹æ¯”å­¦ä¹ ï¼‰
            if hasattr(self.model, 'forward') and 'return_embeddings' in self.model.forward.__code__.co_varnames:
                outputs, embeddings = self.model(data, return_embeddings=True)
                
                # æ··åˆæŸå¤±ï¼šåˆ†ç±»æŸå¤± + å¯¹æ¯”å­¦ä¹ æŸå¤±
                focal_loss = self.focal_loss(outputs, data.y)
                contrastive_loss = self.contrastive_loss(embeddings, data.y)
                
                # åŠ¨æ€æƒé‡ï¼ˆéšè®­ç»ƒè¿›è¡Œè°ƒæ•´å¯¹æ¯”å­¦ä¹ æƒé‡ï¼‰
                contrastive_weight = max(0.1, 1.0 - epoch / 100)  # éšè®­ç»ƒå‡å°‘å¯¹æ¯”å­¦ä¹ æƒé‡
                
                loss = focal_loss + contrastive_weight * contrastive_loss
            else:
                # ä¼ ç»Ÿæ¨¡å‹åªä½¿ç”¨FocalæŸå¤±
                outputs = self.model(data)
                loss = self.focal_loss(outputs, data.y)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            total_loss += loss.item()
            preds = outputs.argmax(dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(data.y.cpu().numpy())
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        avg_loss = total_loss / len(train_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """
        éªŒè¯æ¨¡å‹
        """
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for data in val_loader:
                data = data.to(self.device)
                outputs = self.model(data)
                loss = self.focal_loss(outputs, data.y)
                
                total_loss += loss.item()
                preds = outputs.argmax(dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(data.y.cpu().numpy())
        
        avg_loss = total_loss / len(val_loader)
        accuracy = accuracy_score(all_labels, all_preds)
        balanced_accuracy = balanced_accuracy_score(all_labels, all_preds)
        
        return avg_loss, accuracy, balanced_accuracy
    
    def train(self, train_loader, val_loader, num_epochs):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹
        """
        logger.info("ğŸš€ å¼€å§‹å…ˆè¿›è®­ç»ƒæµç¨‹...")
        
        best_val_acc = 0
        best_model_state = None
        
        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_acc = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯é˜¶æ®µ
            val_loss, val_acc, val_balanced_acc = self.validate(val_loader)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            self.train_history['train_loss'].append(train_loss)
            self.train_history['train_acc'].append(train_acc)
            self.train_history['val_loss'].append(val_loss)
            self.train_history['val_acc'].append(val_acc)
            self.train_history['learning_rates'].append(current_lr)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = self.model.state_dict().copy()
            
            # æ—¥å¿—è¾“å‡º
            if epoch % 10 == 0 or epoch < 10:
                logger.info(f"Epoch {epoch:3d} | "
                          f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
                          f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
                          f"Val Balanced Acc: {val_balanced_acc:.4f} | LR: {current_lr:.6f}")
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping and self.early_stopping(val_loss, self.model):
                logger.info(f"æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒã€‚æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break
        
        # æ¢å¤æœ€ä½³æ¨¡å‹
        if best_model_state is not None:
            self.model.load_state_dict(best_model_state)
            logger.info(f"æ¢å¤æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
        
        return self.train_history


def create_advanced_model(config, device):
    """
    åˆ›å»ºå…ˆè¿›çš„è„‘çŠ¶æ€åˆ†ç±»æ¨¡å‹
    """
    # æš‚æ—¶ä½¿ç”¨æ”¹è¿›çš„ä¼ ç»Ÿæ¨¡å‹ï¼Œé¿å…å¤æ‚çš„ç»´åº¦é—®é¢˜
    # ä½†é›†æˆå…ˆè¿›çš„è®­ç»ƒæŠ€æœ¯
    model = MultiLayerGCN(
        dropout=config.DROPOUT_RATE,
        num_classes=config.NUM_CLASSES
    )
    
    logger.info(f"åˆ›å»ºæ”¹è¿›æ¨¡å‹ï¼Œå‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters())}")
    logger.info("ä½¿ç”¨MultiLayerGCN + å…ˆè¿›è®­ç»ƒæŠ€æœ¯çš„ç»„åˆ")
    return model


def set_random_seeds():
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    """
    torch.manual_seed(config.TORCH_SEED)
    torch.cuda.manual_seed_all(config.TORCH_SEED)
    np.random.seed(config.RANDOM_SEED)
    random.seed(config.RANDOM_SEED)


def generate_detailed_report(true_labels, predictions, num_classes=6):
    """
    ç”Ÿæˆè¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    
    Parameters
    ----------
    true_labels : np.ndarray
        çœŸå®æ ‡ç­¾
    predictions : np.ndarray
        é¢„æµ‹æ ‡ç­¾
    num_classes : int
        ç±»åˆ«æ•°é‡
        
    Returns
    -------
    str
        è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    """
    target_names = [f'çŠ¶æ€_{i}' for i in range(num_classes)]
    report = classification_report(
        true_labels, 
        predictions, 
        target_names=target_names,
        digits=4
    )
    
    return report


def main():
    """
    ä¸»å‡½æ•° - å¢å¼ºç‰ˆè®­ç»ƒæµç¨‹
    """
    logger.info("="*80)
    logger.info("ğŸ§  è„‘ç½‘ç»œçŠ¶æ€åˆ†ç±»å™¨ - å…ˆè¿›ç‰ˆæœ¬")
    logger.info("="*80)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    try:
        # åŠ è½½æ•°æ®é›†
        logger.info("ğŸ“Š åŠ è½½æ•°æ®é›†...")
        train_loader, val_loader, test_loader = get_dataset()
        logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆ - è®­ç»ƒé›†: {len(train_loader)}, éªŒè¯é›†: {len(val_loader)}, æµ‹è¯•é›†: {len(test_loader)}")
        
        # åˆ›å»ºå…ˆè¿›æ¨¡å‹
        logger.info("ğŸ—ï¸  åˆ›å»ºå…ˆè¿›æ¨¡å‹...")
        model = create_advanced_model(config, device)
        
        # åˆ›å»ºè‡ªé€‚åº”è®­ç»ƒå™¨
        trainer = AdaptiveTrainer(model, device, config)
        
        # è®­ç»ƒæ¨¡å‹
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        train_history = trainer.train(train_loader, val_loader, config.NUM_EPOCHS)
        
        # æµ‹è¯•æœ€ç»ˆæ€§èƒ½
        logger.info("ğŸ” æµ‹è¯•æœ€ç»ˆæ€§èƒ½...")
        test_loss, test_acc, test_balanced_acc = trainer.validate(test_loader)
        
        logger.info("="*60)
        logger.info("ğŸ“Š æœ€ç»ˆç»“æœ:")
        logger.info(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        logger.info(f"å¹³è¡¡å‡†ç¡®ç‡: {test_balanced_acc:.4f}")
        logger.info(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
        logger.info("="*60)
        
        # ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒå†å²åˆ°resultsæ–‡ä»¶å¤¹
        # ç¡®ä¿resultsç›®å½•å­˜åœ¨
        config.RESULT_DIR.mkdir(exist_ok=True)
        
        model_save_path = config.RESULT_DIR / 'advanced_brain_classifier.pth'
        torch.save({
            'model_state_dict': model.state_dict(),
            'train_history': train_history,
            'test_accuracy': test_acc,
            'test_balanced_accuracy': test_balanced_acc,
            'config': {
                'num_epochs': config.NUM_EPOCHS,
                'learning_rate': config.LEARNING_RATE,
                'dropout_rate': config.DROPOUT_RATE,
                'num_classes': config.NUM_CLASSES,
                'use_early_stopping': config.USE_EARLY_STOPPING,
                'early_stopping_patience': config.EARLY_STOPPING_PATIENCE,
            },
            'model_info': {
                'model_type': 'MultiLayerGCN',
                'total_parameters': sum(p.numel() for p in model.parameters()),
                'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),
            }
        }, model_save_path)
        
        logger.info(f"âœ… æ¨¡å‹å’Œç»“æœå·²ä¿å­˜åˆ°: {model_save_path}")
        logger.info(f"ğŸ“ ç»“æœç›®å½•: {config.RESULT_DIR}")
        
        return test_acc
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return 0


if __name__ == '__main__':
    try:
        # éªŒè¯é…ç½®
        config.validate_config()
        
        # è¿è¡Œä¸»ç¨‹åº
        main()
        
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

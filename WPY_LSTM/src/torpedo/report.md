# LSTM 模型开发与优化过程总结

## 1. 引言

本文档旨在总结针对神经元活动数据与行为标签分类任务的 LSTM 模型开发和优化过程。目标是构建一个能够准确识别不同行为状态的模型。整个过程涉及模型设计、代码重构、验证策略调整、以及针对性能问题的多轮迭代调试。

## 2. 初始阶段：模型构建与代码重构

*   **初始模型设计:** 最初构想了一个相对复杂的 `EnhancedNeuronLSTM` 模型，整合了：
    *   自编码器 (AE) 用于特征降维或表征学习。
    *   双向 LSTM (BiLSTM) 用于捕捉时序依赖。
    *   多头注意力 (Multi-Head Attention) 和时间注意力 (Temporal Attention) 机制，期望增强模型对关键时间点或特征的关注。
*   **代码模块化:** 为了提高代码的可维护性和可重用性，进行了代码重构，将核心逻辑拆分到以下文件中：
    *   `torpedo/config.py`: 集中管理所有配置参数（路径、超参数等）。
    *   `torpedo/data_utils.py`: 包含数据加载、预处理（`NeuronDataProcessor`）、数据集封装（`NeuronDataset`）、数据分割逻辑等。
    *   `torpedo/model.py`: 包含模型及其子模块（AE, Attention, LSTM, Classifier）的定义。
    *   `torpedo/train.py`: 包含训练循环、验证逻辑和 K-Fold 交叉验证的主流程。
    *   `torpedo/evaluate_lstm.py`: 用于在测试集上评估最终模型。
    *   `torpedo/report_utils.py`: 用于生成 Markdown 格式的总结报告。
    *   `streamlit_app.py`: 提供一个交互式前端界面来运行训练和评估。

## 3. K-Fold 交叉验证与初步调试

*   **引入 K-Fold:** 为了更可靠地评估模型性能并充分利用数据，我们引入了基于行为片段的 Stratified K-Fold 交叉验证。
*   **问题 1: 初步 K-Fold 结果不佳:**
    *   **现象:** 最初的 K-Fold 结果显示平均验证准确率较低（例如 ~56%），且 Fold 间的准确率差异很大，表明模型训练不稳定且泛化能力差。
    *   **尝试 1 (分割长片段):** 怀疑是行为片段长度不一导致问题，尝试将过长的片段分割成固定长度的子片段（`max_episode_len=100`）。**结果:** 性能**显著恶化**，可能是因为强制分割破坏了重要的长时序信息。
    *   **尝试 2 (原始片段 + 时长分层):** 恢复使用原始、未经分割的行为片段。同时，改进 K-Fold 分层策略，从简单地平衡片段*数量*改为平衡每个类别片段的*总时长*（总样本点数），期望更公平地分配数据。**结果:** 性能依然很差（~52%），且观察到**严重的过拟合**现象（训练准确率很高，验证准确率在早期就达到峰值然后骤降）。

## 4. 深入调试：数据处理与过拟合问题

*   **问题 2: 严重的过拟合 (使用原始片段):** 即使使用了原始片段和时长分层，模型仍然快速过拟合。
    *   **尝试 3 (边界序列过滤):** 诊断发现，K-Fold 分割后，将不同片段的数据拼接起来送入 `NeuronDataset` 时，滑动窗口会产生跨越原始片段边界的"假"序列。这些序列混合了不相关行为的信息，可能干扰了学习。因此，在 `train.py` 和 `evaluate_lstm.py` 中加入了**边界序列过滤**逻辑，确保送入模型的每个序列都完全来自同一个原始行为片段。**结果:** 过拟合问题**依然存在** (平均准确率 ~52%)。
    *   **尝试 4 (类别权重):** 观察到训练过程中损失有时停滞不降，怀疑是数据中常见的**类别不平衡**问题导致。在 `train.py` 中加入了逻辑，为每个 Fold 的训练集计算类别权重，并将其传递给 `CrossEntropyLoss`。**结果:** 平均准确率略有提升（~55%），但**核心的过拟合问题未解决**。
    *   **尝试 5 (调整优化参数):** 怀疑学习率 (`0.0005`) 可能过高，正则化（`weight_decay=1e-3`）不足。将学习率显著降低至 `0.0001`，并将权重衰减增加至 `5e-3`。**结果:** 平均准确率变化不大（~58%），**严重的过拟合依然是主要问题**。
    *   **尝试 6 (降低模型复杂度):** 既然调整优化策略效果有限，推断模型对于当前数据可能过于复杂。将 `num_layers` 从 3 降至 1，`hidden_size` 从 256 降至 128。**结果:** 平均准确率无明显改善（~55%），**过拟合问题仍未解决**。

## 5. 突破：增加序列长度

*   **尝试 7 (增加序列长度):** 在排除了多种可能性后，考虑到行为识别可能需要更长的上下文信息，我们将 `sequence_length` 从 10 **增加到 30**，同时保持较低的模型复杂度（`num_layers=1`, `hidden_size=128`）。
*   **结果与发现:**
    *   **显著改进:** K-Fold 平均最佳验证准确率大幅提升至 **66.67%**。
    *   **稳定性提高:** Fold 间的准确率标准差显著减小（降至 5.41%）。
    *   **过拟合缓解:** 过拟合现象虽然仍然存在，但发生时间有所推迟，不再像之前那样在第一轮就崩溃。
    *   **关键结论:** **序列长度 (`sequence_length=30`) 是改善模型性能的关键因素**，结合之前的各项修正（简化模型、边界过滤、类别加权、优化参数调整）共同作用，带来了积极效果。

## 6. 工具链更新与最终评估问题

*   **更新报告工具 (`report_utils.py`):** 为了更好地追踪实验，更新了报告生成逻辑，特别是 `append_kfold_summary` 函数，使其在报告中包含每次 K-Fold 运行时使用的关键配置参数。
*   **同步评估脚本 (`evaluate_lstm.py`):**
    *   确保评估脚本使用与训练时一致的简化模型定义。
    *   加入了与训练脚本相同的边界序列过滤逻辑。
    *   解决了 `ModuleNotFoundError`（通过创建 `__init__.py` 和调整脚本运行方式）和 `TypeError`（修正耗时格式化）等问题。
*   **训练最终模型:** 在 `train.py` 中加入了 K-Fold 循环后的逻辑：使用当前最佳配置，在完整训练集上训练一个最终模型，并保存该模型及对应的 Scaler 以供评估。
*   **问题 3: 测试集类别缺失:**
    *   **现象:** 运行最终评估后，发现测试集评估报告中 'Open' 类别的样本数 (support) 为 0。
    *   **诊断:** 通过在 `evaluate_lstm.py` 中添加详细日志，确认问题出在 `split_data` 函数。由于采用严格的时间顺序分割，并且 'Open' 类的样本在原始数据的时间轴末端恰好没有出现，导致最终划分出的测试集完全不包含 'Open' 类样本。
    *   **影响:** 当前的测试集准确率（~61.6%）仅反映了模型区分 'Closed' 和 'Middle' 的能力，并非完整的三分类性能评估。

## 7. 当前状态与后续方向

*   **当前状态:**
    *   通过一系列迭代优化，特别是增加序列长度至 30，显著提升了模型的 K-Fold 验证性能（平均准确率 ~66.7%）和稳定性。
    *   模型结构已简化为 BiLSTM + Classifier。
    *   训练和评估流程包含了必要的边界序列过滤和类别加权。
    *   报告工具得到了增强，可以记录关键配置。
    *   主要遗留问题是**测试集的代表性不足**（因时间分割导致缺少 'Open' 类），使得当前的测试集评估结果有局限性。
*   **后续建议:**
    *   **短期:** 接受当前测试集评估的局限性，在报告中清晰说明，并将 K-Fold 结果作为模型在三分类任务上潜力的主要参考。
    *   **中期 (可选):**
        *   微调超参数：在当前基础上小幅调整 `hidden_size`、`num_layers`、`sequence_length` 或 `dropout`，观察是否能进一步提升 K-Fold 性能。
        *   修改数据分割策略 (`split_data`)，例如牺牲部分时间顺序以保证测试集的类别完整性（如分层抽样），但这需要谨慎评估对时序模型评估有效性的影响。
    *   **长期 (如果 LSTM 性能仍不理想):**
        *   探索其他模型架构，如 TCN 或基于 Transformer 的模型。
        *   进行更深入的特征工程，提取可能更有区分度的特征。

## 8. 总结

本次开发过程是一个典型的机器学习项目迭代过程。我们从一个复杂的初始模型出发，通过模块化重构、引入 K-Fold 验证，并系统地诊断和解决了数据处理（片段分割、边界效应、类别不平衡）、模型配置（复杂度、优化参数）等多个层面的问题。最终，增加输入序列长度被证明是提升模型性能的关键一步。虽然测试集评估因数据分割策略遇到了新的挑战，但 K-Fold 结果表明当前模型已具备一定的行为分类能力。后续工作应在清晰报告当前评估局限性的基础上，考虑进一步的模型微调或架构探索。

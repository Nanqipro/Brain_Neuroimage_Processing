#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è°ƒè¯•è„šæœ¬

ç”¨äºæ£€æŸ¥æ•°æ®ä¸­æ ‡ç­¾çš„åˆ†å¸ƒï¼Œæ‰¾å‡ºå¯¼è‡´CUDAé”™è¯¯çš„åŸå› 
"""

import torch
import numpy as np
import pandas as pd
import os
from utils import get_dataset


def check_data_integrity(data_path='./data'):
    """
    æ£€æŸ¥æ•°æ®å®Œæ•´æ€§å’Œæ ‡ç­¾åˆ†å¸ƒ
    
    Parameters
    ----------
    data_path : str
        æ•°æ®è·¯å¾„
    """
    print("=== æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ ===")
    
    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = ['nodes.csv', 'edges.csv', 'graphs.csv']
    for file in required_files:
        filepath = os.path.join(data_path, file)
        if os.path.exists(filepath):
            print(f"âœ“ {file} å­˜åœ¨")
        else:
            print(f"âœ— {file} ä¸å­˜åœ¨")
            return False
    
    # è¯»å–å’Œæ£€æŸ¥graphs.csvä¸­çš„æ ‡ç­¾
    print("\n=== æ£€æŸ¥graphs.csvä¸­çš„æ ‡ç­¾ ===")
    graphs_df = pd.read_csv(os.path.join(data_path, 'graphs.csv'))
    print(f"å›¾æ•°é‡: {len(graphs_df)}")
    print(f"æ ‡ç­¾åˆ—: {graphs_df.columns.tolist()}")
    
    if 'label' in graphs_df.columns:
        labels = graphs_df['label'].values
        print(f"æ ‡ç­¾èŒƒå›´: [{labels.min()}, {labels.max()}]")
        print(f"æ ‡ç­¾ç±»å‹: {labels.dtype}")
        print(f"å”¯ä¸€æ ‡ç­¾å€¼: {np.unique(labels)}")
        print(f"æ ‡ç­¾åˆ†å¸ƒ:")
        for label in np.unique(labels):
            count = np.sum(labels == label)
            print(f"  æ ‡ç­¾ {label}: {count} ä¸ªæ ·æœ¬")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆæ ‡ç­¾
        invalid_labels = (labels < 0) | (labels >= 6)
        if np.any(invalid_labels):
            print(f"âš ï¸  å‘ç° {np.sum(invalid_labels)} ä¸ªæ— æ•ˆæ ‡ç­¾ï¼")
            print(f"æ— æ•ˆæ ‡ç­¾å€¼: {labels[invalid_labels]}")
            return False
        else:
            print("âœ“ æ‰€æœ‰æ ‡ç­¾éƒ½åœ¨æœ‰æ•ˆèŒƒå›´ [0, 5] å†…")
    
    return True


def check_dataloader_labels():
    """
    æ£€æŸ¥DataLoaderä¸­çš„æ ‡ç­¾
    """
    print("\n=== æ£€æŸ¥DataLoaderä¸­çš„æ ‡ç­¾ ===")
    
    try:
        # åŠ è½½æ•°æ®é›†
        train_dataloader, valid_dataloader, test_dataloader = get_dataset('./data')
        
        # æ£€æŸ¥è®­ç»ƒé›†æ ‡ç­¾
        print("æ£€æŸ¥è®­ç»ƒé›†æ ‡ç­¾...")
        train_labels = []
        for i, batch in enumerate(train_dataloader):
            y = batch[1]  # æ ‡ç­¾
            train_labels.extend(y.numpy())
            if i >= 5:  # åªæ£€æŸ¥å‰å‡ ä¸ªbatch
                break
        
        train_labels = np.array(train_labels)
        print(f"è®­ç»ƒé›†æ ‡ç­¾èŒƒå›´: [{train_labels.min()}, {train_labels.max()}]")
        print(f"è®­ç»ƒé›†å”¯ä¸€æ ‡ç­¾: {np.unique(train_labels)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆæ ‡ç­¾
        invalid_train = (train_labels < 0) | (train_labels >= 6)
        if np.any(invalid_train):
            print(f"âš ï¸  è®­ç»ƒé›†ä¸­å‘ç° {np.sum(invalid_train)} ä¸ªæ— æ•ˆæ ‡ç­¾ï¼")
            print(f"æ— æ•ˆæ ‡ç­¾å€¼: {train_labels[invalid_train]}")
            return False
        
        # æ£€æŸ¥éªŒè¯é›†æ ‡ç­¾
        print("æ£€æŸ¥éªŒè¯é›†æ ‡ç­¾...")
        valid_labels = []
        for i, batch in enumerate(valid_dataloader):
            y = batch[1]
            valid_labels.extend(y.numpy())
            if i >= 5:
                break
        
        valid_labels = np.array(valid_labels)
        print(f"éªŒè¯é›†æ ‡ç­¾èŒƒå›´: [{valid_labels.min()}, {valid_labels.max()}]")
        print(f"éªŒè¯é›†å”¯ä¸€æ ‡ç­¾: {np.unique(valid_labels)}")
        
        # æ£€æŸ¥æµ‹è¯•é›†æ ‡ç­¾
        print("æ£€æŸ¥æµ‹è¯•é›†æ ‡ç­¾...")
        test_labels = []
        for i, batch in enumerate(test_dataloader):
            y = batch[1]
            test_labels.extend(y.numpy())
            if i >= 5:
                break
        
        test_labels = np.array(test_labels)
        print(f"æµ‹è¯•é›†æ ‡ç­¾èŒƒå›´: [{test_labels.min()}, {test_labels.max()}]")
        print(f"æµ‹è¯•é›†å”¯ä¸€æ ‡ç­¾: {np.unique(test_labels)}")
        
        print("âœ“ DataLoaderæ ‡ç­¾æ£€æŸ¥å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âœ— DataLoaderæ£€æŸ¥å¤±è´¥: {e}")
        return False


def fix_labels_if_needed(data_path='./data'):
    """
    å¦‚æœå‘ç°æ ‡ç­¾é—®é¢˜ï¼Œå°è¯•ä¿®å¤
    """
    print("\n=== å°è¯•ä¿®å¤æ ‡ç­¾é—®é¢˜ ===")
    
    graphs_file = os.path.join(data_path, 'graphs.csv')
    if not os.path.exists(graphs_file):
        print("graphs.csvæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•ä¿®å¤")
        return False
    
    # è¯»å–graphs.csv
    graphs_df = pd.read_csv(graphs_file)
    
    if 'label' not in graphs_df.columns:
        print("æ²¡æœ‰æ‰¾åˆ°labelåˆ—")
        return False
    
    original_labels = graphs_df['label'].values
    print(f"åŸå§‹æ ‡ç­¾èŒƒå›´: [{original_labels.min()}, {original_labels.max()}]")
    
    # ä¿®å¤æ— æ•ˆæ ‡ç­¾
    fixed_labels = np.clip(original_labels, 0, 5)  # å°†æ ‡ç­¾é™åˆ¶åœ¨[0, 5]èŒƒå›´å†…
    
    # å¦‚æœæœ‰è´Ÿæ•°ï¼Œè®¾ä¸º0ï¼›å¦‚æœè¶…è¿‡5ï¼Œè®¾ä¸º5
    graphs_df['label'] = fixed_labels
    
    # ä¿å­˜ä¿®å¤åçš„æ–‡ä»¶
    backup_file = graphs_file + '.backup'
    graphs_df_original = pd.read_csv(graphs_file)
    graphs_df_original.to_csv(backup_file, index=False)
    print(f"åŸå§‹æ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
    
    graphs_df.to_csv(graphs_file, index=False)
    print(f"ä¿®å¤åçš„æ ‡ç­¾èŒƒå›´: [{fixed_labels.min()}, {fixed_labels.max()}]")
    print(f"æ ‡ç­¾å·²ä¿®å¤å¹¶ä¿å­˜åˆ°: {graphs_file}")
    
    return True


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ•°æ®è¯Šæ–­...\n")
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    if not check_data_integrity():
        print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥")
        
        # å°è¯•ä¿®å¤æ ‡ç­¾
        if fix_labels_if_needed():
            print("æ ‡ç­¾å·²ä¿®å¤ï¼Œè¯·é‡æ–°è¿è¡Œæ•°æ®æ£€æŸ¥")
        else:
            print("æ ‡ç­¾ä¿®å¤å¤±è´¥")
        return
    
    # æ£€æŸ¥DataLoader
    if not check_dataloader_labels():
        print("DataLoaderæ£€æŸ¥å¤±è´¥")
        return
    
    print("\nğŸ‰ æ•°æ®æ£€æŸ¥å®Œæˆï¼Œæ‰€æœ‰æ ‡ç­¾éƒ½åœ¨æ­£ç¡®èŒƒå›´å†…ï¼")
    print("ç°åœ¨å¯ä»¥å®‰å…¨è¿è¡Œ main.py")


if __name__ == "__main__":
    main() 